{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNXco6kC0TBS7Du2mGpX9cT"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "WuJONVucbhXu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "92Xar1R0bant"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install sacrebleu"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y transformers accelerate\n",
        "!pip install transformers accelerate"
      ],
      "metadata": {
        "id": "u1HutjhebjUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, load_metric, Dataset\n",
        "from transformers import EncoderDecoderModel\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq\n",
        "\n",
        "encoder_tokenizer = AutoTokenizer.from_pretrained('SIKU-BERT/sikuroberta')\n",
        "decoder_tokenizer= AutoTokenizer.from_pretrained('hfl/chinese-roberta-wwm-ext')\n"
      ],
      "metadata": {
        "id": "H7fPkRHabln9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/drive/MyDrive/Colab Notebooks/train_24-historoes_c_utf8.txt\", 'r', encoding='utf-8') as f:\n",
        "    lines1 = [line.strip() for line in f]\n",
        "    train_size = int(len(lines1) * 0.995)\n",
        "    train1 = lines1[:train_size]\n",
        "    val1 = lines1[train_size:]\n",
        "with open(\"/content/drive/MyDrive/Colab Notebooks/train_24_histories_m_utf8.txt\", 'r', encoding='utf-8') as f:\n",
        "    lines2 = [line.strip() for line in f]\n",
        "    train2 = lines2[:train_size]\n",
        "    val2 = lines2[train_size:]\n",
        "\n",
        "train_data=Dataset.from_dict({\"source\": train1, \"target\": train2})\n",
        "val_data=Dataset.from_dict({\"source\": val1, \"target\": val2})"
      ],
      "metadata": {
        "id": "rNACSIb4cPQz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_max_length=192\n",
        "decoder_max_length=192\n",
        "\n",
        "def process_data_to_model_inputs(batch):\n",
        "  # tokenize the inputs and labels\n",
        "  inputs = encoder_tokenizer(batch[\"source\"], padding=\"max_length\", truncation=True, max_length=encoder_max_length)\n",
        "  outputs = decoder_tokenizer(batch[\"target\"], padding=\"max_length\", truncation=True, max_length=decoder_max_length)\n",
        "\n",
        "  batch[\"input_ids\"] = inputs.input_ids\n",
        "  batch[\"attention_mask\"] = inputs.attention_mask\n",
        "  batch[\"decoder_attention_mask\"] = outputs.attention_mask\n",
        "  batch[\"labels\"] = outputs.input_ids.copy()\n",
        "\n",
        "  # because BERT automatically shifts the labels, the labels correspond exactly to `decoder_input_ids`.\n",
        "  # We have to make sure that the PAD token is ignored\n",
        "  batch[\"labels\"] = [[-100 if token == decoder_tokenizer.pad_token_id else token for token in labels] for labels in batch[\"labels\"]]\n",
        "  return batch"
      ],
      "metadata": {
        "id": "D-i5IExqcSAU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 28\n",
        "# batch_size=4\n",
        "\n",
        "train_data = train_data.map(\n",
        "    process_data_to_model_inputs,\n",
        "    batched=True,\n",
        "    batch_size=batch_size,\n",
        "    remove_columns=[\"source\", \"target\"]\n",
        ")\n",
        "\n",
        "val_data = val_data.map(\n",
        "    process_data_to_model_inputs,\n",
        "    batched=True,\n",
        "    batch_size=batch_size,\n",
        "    remove_columns=[\"source\", \"target\"]\n",
        ")"
      ],
      "metadata": {
        "id": "Zayfc59nctXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.set_format(\n",
        "    type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"decoder_attention_mask\", \"labels\"],\n",
        ")\n",
        "\n",
        "val_data.set_format(\n",
        "    type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"decoder_attention_mask\", \"labels\"],\n",
        ")"
      ],
      "metadata": {
        "id": "mxJiORJ6cvdk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "#from transformers import EncoderDecoderConfig, RobertaConfig\n",
        "\n",
        "#encoder_config = RobertaConfig.from_pretrained('roberta-large', dropout=0.2)\n",
        "#decoder_config = RobertaConfig.from_pretrained('roberta-large', dropout=0.2)\n",
        "\n",
        "#config = EncoderDecoderConfig.from_encoder_decoder_configs(encoder_config, decoder_config, tie_encoder_decoder=True)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "WcaKaVHmSYyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model = EncoderDecoderModel(config=config)\n",
        "model = EncoderDecoderModel.from_encoder_decoder_pretrained('SIKU-BERT/sikuroberta','SIKU-BERT/sikuroberta',tie_encoder_decoder=True)\n",
        "model.config.decoder_start_token_id = decoder_tokenizer.cls_token_id\n",
        "model.config.eos_token_id = decoder_tokenizer.sep_token_id\n",
        "model.config.pad_token_id = decoder_tokenizer.pad_token_id\n",
        "model.config.vocab_size = model.config.encoder.vocab_size\n",
        "\n",
        "model.config.max_length = 192\n",
        "model.config.min_length = 6\n",
        "model.config.no_repeat_ngram_size = 3\n",
        "model.config.early_stopping = True\n",
        "model.config.length_penalty = 2.0\n",
        "#model.config.repetition_penalty = 1.2\n",
        "model.config.num_beams = 4\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    predict_with_generate=True,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    save_strategy='steps',\n",
        "    eval_steps=2000,\n",
        "    save_steps=4000,\n",
        "    output_dir=\"./\",\n",
        "    fp16=True,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    learning_rate=2e-5,\n",
        "    #weight_decay=0.005,\n",
        "    num_train_epochs=3,\n",
        "    warmup_steps=2000\n",
        ")"
      ],
      "metadata": {
        "id": "ad7oyV78czZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load bleu for validation\n",
        "bleu = load_metric(\"sacrebleu\")\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def postprocess_text(preds, labels):\n",
        "    preds = [pred.strip() for pred in preds]\n",
        "    labels = [[label.strip()] for label in labels]\n",
        "\n",
        "    return preds, labels\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    preds, labels = eval_preds\n",
        "    if isinstance(preds, tuple):\n",
        "        preds = preds[0]\n",
        "    decoded_preds = decoder_tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "\n",
        "    # Replace -100 in the labels as we can't decode them.\n",
        "    labels = np.where(labels != -100, labels, decoder_tokenizer.pad_token_id)\n",
        "    decoded_labels = decoder_tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    # Some simple post-processing\n",
        "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
        "\n",
        "    result = bleu.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "    result = {\"bleu\": result[\"score\"]}\n",
        "\n",
        "    result = {k: round(v, 4) for k, v in result.items()}\n",
        "    return result"
      ],
      "metadata": {
        "id": "d_WCt0jdSla2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorForSeq2Seq(decoder_tokenizer, model=model)\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model,\n",
        "    training_args,\n",
        "    train_dataset=train_data,\n",
        "    eval_dataset=val_data,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=decoder_tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "trainer.save_model('test_model')"
      ],
      "metadata": {
        "id": "MjHGAO1DSpsg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
